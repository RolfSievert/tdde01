# Assignment 2

## Step 1

Firstly, the data was separated into training, validation and test data.

```{r echo = FALSE}
# Add libraries
library("gridExtra")
library("ggplot2")
library("tree")
library("MASS")
library("e1071")

# Set working directory
setwd("~/courses/tdde01/lab2")

# Read data
scores = read.csv2("creditscoring.csv")
# Read as strings
scores$good_bad = as.factor(scores$good_bad)

# Split data into train/val/test
n=dim(scores)[1]
set.seed(12345) 
id=sample(1:n, floor(n*0.5))
train=scores[id,] 
id1=setdiff(1:n, id)
set.seed(12345) 
id2=sample(id1, floor(n*0.25)) 
val=scores[id2,]
id3=setdiff(id1,id2)
test=scores[id3,]
```

## Step 2

```{r echo = FALSE}
# Create tree
tree.deviance = tree(formula = good_bad~., 
                     data = train, 
                     split="deviance")
tree.gini = tree(formula = good_bad~., 
                     data = train, 
                     split="gini")

# Make predictions
predict.deviance.test = predict(tree.deviance, test, type = "class")
predict.deviance.train = predict(tree.deviance, train, type = "class")
predict.gini.test = predict(tree.gini, test, type = "class")
predict.gini.train = predict(tree.gini, train, type = "class")

# Calculate misclassification rates
misclass = function(predicted, true) {
    return(mean(predicted != true))
}

deviance.test.misclass = misclass(predict.deviance.test, test$good_bad)
deviance.train.misclass = misclass(predict.deviance.train, train$good_bad)
gini.test.misclass = misclass(predict.gini.test, test$good_bad)
gini.train.misclass = misclass(predict.gini.train, train$good_bad)

# Print misclassification rates
cat("Misclassification on deviance with test: ", 
    deviance.test.misclass)
cat("Misclassification on deviance with train: ", 
    deviance.train.misclass)
cat("Misclassification on gini with test: ", 
    gini.test.misclass)
cat("Misclassification on gini with train: ", 
    gini.train.misclass, "\n") 
```

Here, misclassification rate is lower when using deviance as measure of impurity.

## Step 3

```{r echo = FALSE}
# Lists of scores
range = 15
pruned=rep(0, range)
trainScore=rep(0, range)
testScore=rep(0, range)

test.tree = tree(formula=good_bad~., data=train)
for(i in 2:range) {
    # Prune the tree
    prunedTree=prune.tree(test.tree, best=i)
    # Make trediction on validation data
    pred=predict(prunedTree, newdata=val, type="tree")
    # Append scores
    trainScore[i]=deviance(prunedTree)
    testScore[i]=deviance(pred)
}

# Plot the scores
plot(2:range, trainScore[2:range], type="b", col="red", ylim=c(200, 600))
points(2:range, testScore[2:range], type="b", col="blue")

optLeaves = 4

# Info on optimal tree
optTree = prune.tree(tree.deviance, best=optLeaves)
#summary(optTree)
optTree.predict = predict(optTree, newdata=test, type="class")
optTree.misclass = misclass(optTree.predict, test$good_bad)
cat("Misclassification on optimal tree: ", 
    optTree.misclass)

cat("Tree depth is 3 as can be seen in the plot. \n")
plot(optTree)
cat("Used variables in optimal tree: \n'savings' 'duration' 'history' \n\n")
```

## Step 4

```{r echo = FALSE}
# Create model and classify
model.bayes = naiveBayes(formula = good_bad~., data=train)
predict.bayes.test = predict(model.bayes, newdata=test, type="class")
predict.bayes.train = predict(model.bayes, newdata=train, type="class")

# Print info on classification with prediction on test
table.bayes = table(Predicted=predict.bayes.test, Actual=test$good_bad)
cat("Confusion table of naïve bayes (test):")
print(table.bayes)

misclass.bayes = mean(predict.bayes.test != test$good_bad)
cat("Misclassification with naïve bayes (test): ", misclass.bayes, "\n\n")

# Print info on classification with prediction on test
table.bayes = table(Predicted=predict.bayes.train, Actual=train$good_bad)
cat("Confusion table of naïve bayes (train):")
print(table.bayes)

misclass.bayes = mean(predict.bayes.train != train$good_bad)
cat("Misclassification with naïve bayes (train): ", misclass.bayes, "\n\n")
```

Naïve Bayes has much better result than in step 3.

## Step 5

```{r echo = FALSE}
# Calculate TPR and FPR of optimal tree and bayes
getROC = function(pred, pi) {
    tpr = c()
    fpr = c()
    for (p in pi) {
        # Change probabilities to strings
        tmp = ifelse(pred[,'good'] > p, "good", "bad")
        # Get confusion matrix
        cm = table(predicted=tmp, actual=test$good_bad)
        if('good' %in% rownames(cm)) {
          # Calculate TPR, first dim of cm is predicted
          t = cm['good', 'good'] / sum(cm[,'good'])
          # Calculate FPR
          f = cm['good', 'bad'] / sum(cm[, 'bad'])
          # Append to list of values
          tpr = c(tpr, ifelse(is.finite(t), t, 0))
          fpr = c(fpr, ifelse(is.finite(f), f, 0))
        } else {
          tpr = c(tpr, 0)
          fpr = c(fpr, 0)
        }
    }
    df = data.frame(tpr, fpr)
    return(df)
}

pi = seq(0.05, 0.95, 0.05)
pred.optTree = predict(optTree, newdata=test)
pred.bayes = predict(model.bayes, newdata=test, type='raw')
opt.roc = getROC(pred.optTree, pi)
bayes.roc = getROC(pred.bayes, pi)

#Plot
roc.plot = ggplot(mapping=aes(col=Classifier)) +
    geom_point(data=opt.roc, aes(x=fpr, y=tpr, col="Optimal tree")) +
    geom_line(data=opt.roc, aes(x=fpr, y=tpr, col="Optimal tree")) +
    geom_point(data=bayes.roc, aes(x=fpr, y=tpr, col="Bayes")) +
    geom_line(data=bayes.roc, aes(x=fpr, y=tpr, col="Bayes"))

grid.arrange(roc.plot)
```

Naïve Bayes has better ratio between TPR and FPR.
The only exception is around $\pi = 0.75$, as can be seen in the graph.

## Step 6

Using loss matrix with naïve bayes.

```{r echo = FALSE}
# Create model and classify
model.bayes = naiveBayes(formula = good_bad~., data=train)
predict.bayes.test = predict(model.bayes, newdata=test, type="raw")
predict.bayes.train = predict(model.bayes, newdata=train, type="raw")

# Print info on classification with prediction on test
loss = 10/1
predict.bayes.test.loss = ifelse(predict.bayes.test[, 'good'] / 
                                 predict.bayes.test[, 'bad'] > loss, 
                                 'good', 
                                 'bad')
table.bayes = table(Predicted=predict.bayes.test.loss, Actual=test$good_bad)
cat("Confusion table of naïve bayes (using test data):")
print(table.bayes)

misclass.bayes = mean(predict.bayes.test.loss != test$good_bad)
cat("Misclassification with naïve bayes (using test data): ", misclass.bayes, "\n")

# Print info on classification with prediction on train
predict.bayes.train.loss = ifelse(predict.bayes.train[, 'good'] / 
                                  predict.bayes.train[, 'bad'] > loss, 
                                 'good', 
                                 'bad')
table.bayes = table(Predicted=predict.bayes.train.loss, Actual=train$good_bad)
cat("Confusion table of naïve bayes (using train data): ")
print(table.bayes)

misclass.bayes = mean(predict.bayes.train.loss != train$good_bad)
cat("Misclassification with naïve bayes (using train data): ", misclass.bayes, "\n")
```

The misclassification is much greater. 
But the confusion matrix is more favorable from an economic point of view for a company since less are predicted good that are actually bad.
